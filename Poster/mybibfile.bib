
@incollection{berrah99,
	author = {Berrah, A.-R. and Laboissi\`ere, R.},
    editor = {D. Floreano, J.-D. Nicoud and F. Mondada},
	title = {SpeÂ­cies: An evolutionary model for the emergence of phonetic structures in an artificial society of speech agents},
    booktitle = {Advances in artificial life, lecture notes in artificial intelligence},
    publisher = {Springer Berlin},
	volume = "1674",
    pages = "674-678",
	year = "1999"
}

@book{strutton,
author={Sutton,Richard S. and Barto,Andrew G.},
year={2015},
month={Mar 6,},
title={Reinforcement Learning : An Introduction},
publisher={Bradford Books},
address={Cambridge},
abstract={Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability.The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning.},
isbn={9780262193986},
language={English},
url={http://lib.myilibrary.com?ID=209678},
}

@article{atari_games,
  author    = {Volodymyr Mnih and
               Koray Kavukcuoglu and
               David Silver and
               Alex Graves and
               Ioannis Antonoglou and
               Daan Wierstra and
               Martin A. Riedmiller},
  title     = {Playing Atari with Deep Reinforcement Learning},
  journal   = {CoRR},
  volume    = {abs/1312.5602},
  year      = {2013},
  url       = {http://arxiv.org/abs/1312.5602},
  timestamp = {Wed, 07 Jun 2017 14:43:06 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/MnihKSGAWR13},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@article{borel,
author={Hornik,Kurt and Stinchcombe,Maxwell and White,Halbert},
year={1989},
title={Multilayer feedforward networks are universal approximators},
journal={Neural Networks},
volume={2},
number={5},
pages={359-366},
isbn={0893-6080},
language={English},
url={http://www.sciencedirect.com/science/article/pii/0893608089900208},
}

@InCollection{vanOtterlo2012,
author="van Otterlo, Martijn
and Wiering, Marco",
editor="Wiering, Marco
and van Otterlo, Martijn",
title="{Reinforcement Learning and Markov Decision Processes}",
bookTitle="Reinforcement Learning: State-of-the-Art",
year="2012",
publisher="Springer Berlin Heidelberg",
abstract="Situated in between supervised learning and unsupervised learning, the paradigm of reinforcement learning deals with learning in sequential decision making problems in which there is limited feedback. This text introduces the intuitions and concepts behind Markov decision processes and two classes of algorithms for computing optimal behaviors: reinforcement learning and dynamic programming. First the formal framework of Markov decision process is defined, accompanied by the definition of value functions and policies. The main part of this text deals with introducing foundational classes of algorithms for learning optimal behaviors, based on various definitions of optimality with respect to the goal of learning sequential decisions. Additionally, it surveys efficient extensions of the foundational algorithms, differing mainly in the way feedback given by the environment is used to speed up learning, and in the way they concentrate on relevant parts of the problem. For both model-based and model-free settings these efficient extensions have shown useful in scaling up to larger problems.",
isbn="978-3-642-27645-3",
doi="10.1007/978-3-642-27645-3_1",
url="http://dx.doi.org/10.1007/978-3-642-27645-3_1"
}

@article{BELLMAN,
 ISSN = {00959057, 19435274},
 URL = {http://www.jstor.org/stable/24900506},
 author = {Richard Bellman},
 journal = {Journal of Mathematics and Mechanics},
 number = {5},
 pages = {679-684},
 publisher = {Indiana University Mathematics Department},
 title = {A Markovian Decision Process},
 volume = {6},
 year = {1957}
}

@article{Q-LearningProof,
author={Watkins, Christopher J C H and Dayan,Peter},
year={1992},
month={May 1,},
title={Technical Note: Q-Learning},
journal={Machine Learning},
volume={8},
number={3},
pages={279},
abstract={This paper presents and proves in detail a convergence theorem for Q -learning based on that outlined in Watkins (1989). We show that Q -learning converges to the optimum action-values with probability 1 so long as all actions are repeatedly sampled in all states and the action-values are represented discretely. We also sketch extensions to the cases of non-discounted, but absorbing, Markov environments, and where many Q values can be changed each iteration, rather than just one.},
isbn={0885-6125},
language={English},
}

@incollection{Rumelhart:1986:LIR:104279.104293,
 author = {Rumelhart, D. E. and Hinton, G. E. and Williams, R. J.},
 chapter = {Learning Internal Representations by Error Propagation},
 title = {Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Vol. 1},
 editor = {Rumelhart, David E. and McClelland, James L. and PDP Research Group, CORPORATE},
 year = {1986},
 isbn = {0-262-68053-X},
 pages = {318--362},
 numpages = {45},
 url = {http://dl.acm.org/citation.cfm?id=104279.104293},
 acmid = {104293},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 

@INPROCEEDINGS{msPacman,
author={L. Bom and R. Henken and M. Wiering}, 
booktitle={2013 IEEE Symposium on Adaptive Dynamic Programming and Reinforcement Learning (ADPRL)}, 
title="{Reinforcement learning to train Ms. Pac-Man using higher-order action-relative inputs}", 
year={2013}, 
pages={156-163}, 
keywords={computer games;feature extraction;learning (artificial intelligence);neural nets;Q-learning;arcade video game Ms. Pac-Man;dynamic environments;game state;higher order action relative inputs;neural network;open research question;reinforcement learning algorithms;smart feature extraction algorithms;train Ms. Pac-Man;Biological neural networks;Games;Heuristic algorithms;Learning (artificial intelligence);Neurons;Training}, 
doi={10.1109/ADPRL.2013.6615002}, 
ISSN={2325-1824},
}

@inproceedings{starcraft,
  title={Connectionist reinforcement learning for intelligent unit micro management in starcraft},
  author={Shantia, Amirhosein and Begue, Eric and Wiering, Marco},
  booktitle={Neural Networks (IJCNN), The 2011 International Joint Conference on},
  pages={1794--1801},
  year={2011},
  organization={IEEE}
}

@InCollection{Q-LearningInGames,
title = "Reinforcement Learning: State of the Art",
keywords = "Reinforcement learning",
author = "Marco Wiering and {Van Otterlo}, Martijn",
year = "2012",
doi = "10.1007/978-3-642-27645-3",
isbn = "978-3-642-27644-6",
publisher = "Springer",
}

@inbook{kaelbling1993learning,
  title={Learning in Embedded Systems},
  author={Kaelbling, L.P.},
  isbn={9780262111744},
  lccn={lc92024672},
  series={A Bradford book},
  url={https://books.google.nl/books?id=WiN53ZYd0kgC},
  year={1993},
  publisher={MIT Press},
}

@article{HQ-learning,
  title="{HQ-learning}",
  author={Wiering, Marco and Schmidhuber, J{\"u}rgen},
  journal={Adaptive Behavior},
  volume={6},
  number={2},
  pages={219--246},
  year={1997},
  publisher={Sage Publications Sage CA: Thousand Oaks, CA}
}


@inproceedings{tijsma2016comparing,
  title={Comparing exploration strategies for {Q-learning} in random stochastic mazes},
  author={Tijsma, Arryon D and Drugan, Madalina M and Wiering, Marco A},
  booktitle={Computational Intelligence (SSCI), 2016 IEEE Symposium Series on},
  pages={1--8},
  year={2016},
}

@phdthesis{wiering1999explorations,
  title={Explorations in efficient reinforcement learning},
  author={Wiering, Marco A},
  year={1999},
  school={University of Amsterdam}
}


@Article{Sutton1988,
author="Sutton, Richard S.",
title="Learning to predict by the methods of temporal differences",
journal="Machine Learning",
year="1988",
day="01",
volume="3",
number="1",
pages="9--44",
abstract="This article introduces a class of incremental learning procedures specialized for prediction-that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage.",
issn="1573-0565",
doi="10.1007/BF00115009",
url="http://dx.doi.org/10.1007/BF00115009",
}

@InCollection{szita2012reinforcement,
author="Szita, Istv{\'a}n",
editor="Wiering, Marco
and van Otterlo, Martijn",
title="Reinforcement Learning in Games",
bookTitle="Reinforcement Learning: State-of-the-Art",
year="2012",
publisher="Springer Berlin Heidelberg",
pages="539--577",
abstract="Reinforcement learning and games have a long and mutually beneficial common history. From one side, games are rich and challenging domains for testing reinforcement learning algorithms. From the other side, in several games the best computer players use reinforcement learning. The chapter begins with a selection of games and notable reinforcement learning implementations.Without any modifications, the basic reinforcement learning algorithms are rarely sufficient for high-level gameplay, so it is essential to discuss the additional ideas, ways of inserting domain knowledge, implementation decisions that are necessary for scaling up. These are reviewed in sufficient detail to understand their potentials and their limitations. The second part of the chapter lists challenges for reinforcement learning in games, together with a review of proposed solution methods. While this listing has a game-centric viewpoint, and some of the items are specific to games (like opponent modelling), a large portion of this overview can provide insight for other kinds of applications, too. In the third part we review how reinforcement learning can be useful in game development and find its way into commercial computer games. Finally, we provide pointers for more in-depth reviews of specific games and solution approaches.",
isbn="978-3-642-27645-3",
doi="10.1007/978-3-642-27645-3_17",
url="http://dx.doi.org/10.1007/978-3-642-27645-3_17"
}

@incollection{IntervalEstimationWhite,
title = "{Interval Estimation for Reinforcement-Learning Algorithms in Continuous-State Domains}",
author = {White, Martha and Adam White},
booktitle = {Advances in Neural Information Processing Systems 23},
editor = {J. D. Lafferty and C. K. I. Williams and J. Shawe-Taylor and R. S. Zemel and A. Culotta},
pages = {2433--2441},
year = {2010},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/4090-interval-estimation-for-reinforcement-learning-algorithms-in-continuous-state-domains.pdf}
}


